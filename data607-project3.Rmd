---
title: "Most Valued Data Science Skills"
author: "Mael Illien, Dhairav Chhatbar, Charlie Rosemond, Matt Mayor, Bryan Persaud"
date: "10/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(rvest)
library(tidyverse)
```

# Motivation
William Edwards Deming is known for the quote “In God we trust, all others must bring data.” It is clear that data-driven decision making is superior and is in these times a requirement to maintain a competitive edge. Every industry now treats their data as an asset to get insights from and is a foundation in the decision making process. Turning data into insights not only requires appropriate systems in place, but also requires the appropriate subject matter experts to interpret and discover what the data has to say.  

Data Science is the art of analyzing data to gain knowledge in order to make business decisions. Data Science at its core is a union of mathematics, statistics, and programming disciplines. Data Scientists need to possess various skills such as data/quantitative analysis, R/Python programming, Machine Learning, Data Visualization, etc. Some skills may be more important than others depending on the task at hand.  

As future Data Scientists we seek to understand what are some of the most important skills to possess. We will gather this consensus by conducting exploratory analysis on what skills employers are looking for through job listings on LinkedIn and Indeed.   


# Approach
<IN PROGRESS>

# Data Aggregation
```{r}
wlist <- read.csv('https://raw.githubusercontent.com/dhairavc/data607-project3/master/whitelist.csv')
```

The extract_words function will take a link to a particular job post and the selector which contains the list of interest and return a dataframe of words in that job post's skills section and the number of occurences of each particular words.

```{r}
extract_words <- function(link_to_job_page, selector) {
  # download the html and turn it into an XML file with read_html()
  job_page <- read_html(link_to_job_page)
  # extract specific nodes with html_nodes() using css selector
  skills_tag <- html_nodes(job_page, selector)
  # extract content from nodes
  skills_text <- html_text(skills_tag)
  # remove punctuation
  skills_text <- gsub('[[:punct:]]', '', skills_text)
  # split sentences into individual words
  words <- unlist(strsplit(skills_text, " "))
  words <- tolower(words)
  # count the number of occurences of each word
  wordcount <- table(words)
  wordcount_df <- as.data.frame(wordcount)
  return(wordcount_df)
}
```

Given a list of links to job pages, call the extract_words function to get the total word counts from the chosen selector from each each link, aggregate them and return the counts in decreasing order in a data frame.

```{r}
get_word_counts <- function(links_to_jobs, selector) {
  
  # initialize a list 
  counts <- list()
  
  for (i in 1:length(links_to_jobs)) {
    df <- extract_words(links_to_jobs[i], selector)
    counts[[i]] <- df
  }
  # combine into a dataframe
  skill_count <- do.call(rbind, counts)
  
  # sum multiple occurences of the same word
  total_skill_count <- skill_count %>% 
    group_by(words) %>% 
    summarize(Occurences = sum(Freq)) %>% 
    arrange(desc(Occurences ))
  return(total_skill_count)
}
```

A function that applies the whitelist of data science key words and skills to our count of word occurences to filter our irrelevant words.

```{r}
get_DS_skills <- function(word_count) {
  # apply whitelist
  total_skill_count <- word_count %>% filter(words %in% wlist$Whitelist)
  return(total_skill_count)
}
```

## Extracting Data Science Skills from Linkedin

```{r}
linkedin_top_skills <- function() {
  # save the url which contains the search results
  rooturl <- "https://www.linkedin.com/jobs/data-scientist-jobs/"
  
  # for each job, extract the href attribute from each job using the css selector
  search_results <- read_html(rooturl)
  links_to_jobs <- search_results %>% 
    html_nodes("a.result-card__full-card-link") %>% 
    html_attr("href")

  for (x in 1:39) {
  paged_url <- "https://www.linkedin.com/jobs/search/?keywords=data%20scientist&start="
  paged_url <- paste(paged_url, 25*x, collapse = NULL, sep = "")
  
  jobs <- read_html(paged_url)
  links_temp <- html_nodes(jobs, "a.result-card__full-card-link")
  links_temp <- html_attr(links_temp, "href")
  links_to_jobs <- c(links_to_jobs, links_temp)

}
  
  # for Linkedin job posts, skills are located in <li> tags with the following selector
  selector <- ".description__text--rich li"
  # get word counts from the list of links to job posts
  word_count <- get_word_counts(links_to_jobs, selector)
  # uncomment this to see the results pre-whitelist
  #print(word_count)
  # get data science related skills from the above word count
  skill_count <- get_DS_skills(word_count)
  return(skill_count)
}
```



## Extracting Data Science Skills from Indeed

```{r}
indeed_top_skills <- function() {
  # save the url which contains the search results
  domain <- "https://www.indeed.com"
  rooturl <- "https://www.indeed.com/jobs?q=data+science&l=New+York+City%2C+NY"
 
   # for each job, extract the href attribute from each job using the css selector
  search_results <- read_html(rooturl)
 
  # create a list of links by extracting the href attribute from the nodes
  paths_to_jobs <- search_results %>%
    html_nodes(".title") %>%
    html_children() %>%
    html_attr("href")
  
    for (x in 1:50) {
   paged_url <- "https://www.indeed.com/jobs?q=data+science&l=New+York+City%2C+NY&start="
   paged_url <- paste(paged_url, 10*x, collapse = NULL, sep = "")
   
   search_results <- read_html(paged_url)
   temp_paths_to_jobs <- search_results %>%
    html_nodes(".title") %>%
    html_children() %>%
    html_attr("href")
   
   paths_to_jobs <- c(paths_to_jobs, temp_paths_to_jobs)
}
  
  # contatenate paths with the domain name to create valid links
  links_to_jobs <- str_c(domain, paths_to_jobs)
  # for Indeed job posts, skills are located in <li> tags so the selector is simple
  selector <- "li"
  # get word counts from the list of links to job posts
  word_count <- get_word_counts(links_to_jobs, selector)
  # uncomment this to see the results pre-whitelist
  #print(word_count)
  # get data science related skills from the above word count
  skill_count <- get_DS_skills(word_count)
  return(skill_count)
}
```

## Web Scraping

```{r}
linkedin <- linkedin_top_skills()
indeed <- indeed_top_skills()
```

# SQL Storage
<IN PROGRESS>

# Analysis & Visualization
<IN PROGRESS>


# Findings & Conclusion
<IN PROGRESS>
