---
title: "linkedin"
author: "Mael Illien"
date: "10/14/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(tidyverse)
```

```{r}
extract_words <- function(link) {
  # download the html and turn it into an XML file with read_html()
  job <- read_html(link)
  # extract specific nodes with html_nodes() using css selector
  skills <- html_nodes(job, ".description__text--rich li")
  # extract content from nodes
  skills <- html_text(skills)
  # remove punctuation
  words <- gsub('[[:punct:]]', '', skills)
  # split sentences into individual words
  words <- unlist(strsplit(words, " "))
  # count the number of occurences of each word
  wordcount <- table(words)
  wordcount_df <- as.data.frame(wordcount)
  return(wordcount_df)
}
```

```{r}
# save the url which contains the search results
rooturl <- "https://www.linkedin.com/jobs/data-scientist-jobs/"
# for each job, extract the href attribute from each job using the css selector
jobs <- read_html(rooturl)
links <- html_nodes(jobs, "a.result-card__full-card-link")
links <- html_attr(links, "href")
```

```{r}
# apply the extract_words function to each link
counts <- list()

for (i in 1:length(links)) {
    df <- extract_words(links[i])
    counts[[i]] <- df
}
# combine into a dataframe
skill_count <- do.call(rbind, counts)
```

```{r}
# sum multiple occurences of the same word
total_skill_count <- skill_count %>% group_by(words) %>% summarize(F = sum(Freq)) %>% arrange(desc(F))
total_skill_count
```
